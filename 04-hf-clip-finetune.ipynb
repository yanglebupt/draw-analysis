{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51abe703-e1de-4eee-b46c-22aca7f43151",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import math\n",
    "import numpy\n",
    "import os\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f2b09cf-8d10-4cf2-9aa4-07a3bb8534d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------#\n",
    "#   设置种子\n",
    "#---------------------------------------------------#\n",
    "def seed_everything(seed=3407):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        \n",
    "def worker_init_fn(worker_id, seed=3407):\n",
    "    worker_seed = worker_id + seed\n",
    "    random.seed(worker_seed)\n",
    "    np.random.seed(worker_seed)\n",
    "    torch.manual_seed(worker_seed)\n",
    "\n",
    "seed=3407\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885912aa-04fd-4e55-9883-a22ef48ece25",
   "metadata": {},
   "source": [
    "## 划分数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab79526f-19de-46b6-9a98-f73280f54a3e",
   "metadata": {},
   "source": [
    "## 加载模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201ad0ca-deff-4b06-b3e2-ca6012e69e45",
   "metadata": {},
   "source": [
    "- LORA 微调 CLIP: https://github.com/kesimeg/LORA-turkish-clip\n",
    "- PEFT https://github.com/datawhalechina/self-llm/blob/master/models/Gemma2/04-Gemma-2-9b-it%20peft%20lora%E5%BE%AE%E8%B0%83.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96475ecd-1ab4-4c73-a39e-fc6f0469d0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import clip\n",
    "\n",
    "# device = 'cuda' if torch.cuda.is_available() else \"cpu\"\n",
    "# model, preprocess = clip.load(\"/root/.cache/torch/hub/checkpoints/ViT-B-16.pt\", device=device, jit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85bd2257-0e49-45fc-8b47-fd612abd84d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "import os\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "\n",
    "save_dir = \"/root/.cache/torch/hub/checkpoints/clip-vit-base-patch16\"\n",
    "\n",
    "# os.makedirs(save_dir, exist_ok=True)\n",
    "# snapshot_download(repo_id=\"openai/clip-vit-base-patch16\", local_dir=save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a16d05e5-211a-4695-9943-987c03037268",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else \"cpu\"\n",
    "processor = CLIPProcessor.from_pretrained(save_dir)\n",
    "pretrained_model = CLIPModel.from_pretrained(save_dir).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "49cb0034-cdc5-439e-951c-42d77cb5a777",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f289250d-1f7d-4663-8b99-5d7296491562",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    ")\n",
    "## 加⼊PEFT策略\n",
    "model = get_peft_model(pretrained_model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8fe73218-2335-4606-90df-52763c93781e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 983040 || all params: 150603777 || trainable%: 0.65\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1b2687-6c72-4411-a788-098e6bc4c061",
   "metadata": {},
   "source": [
    "## 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b2976da-44f2-471f-8dfe-a63b69804fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dataset_folder_info(*dataset_folders):\n",
    "    for dataset_folder in dataset_folders:\n",
    "        print(len(dataset_folder), dataset_folder.classes, dataset_folder.class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c5c46f30-4d3f-43c7-8b9e-db30472ffdbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cats', 'dogs'] {'cats': 0, 'dogs': 1}\n",
      "['cats', 'dogs'] {'cats': 0, 'dogs': 1}\n",
      "['cats', 'dogs'] {'cats': 0, 'dogs': 1}\n"
     ]
    }
   ],
   "source": [
    "dataset_root = \"./dataset\"\n",
    "batch_size = 32\n",
    "num_workers = 4\n",
    "\n",
    "# 加载图像-文本匹配数据集\n",
    "train_folder = MyDataset(datasets.ImageFolder(root=f\"{dataset_root}/train\", transform=transform), texts)\n",
    "val_folder = MyDataset(datasets.ImageFolder(root=f\"{dataset_root}/validation\", transform=transform), texts)\n",
    "test_folder = MyDataset(datasets.ImageFolder(root=f\"{dataset_root}/test\", transform=transform), texts)  # report test-dev\n",
    " \n",
    "# 创建数据加载器\n",
    "train_loader = DataLoader(train_folder, batch_size=batch_size, shuffle=True, num_workers=num_workers, worker_init_fn=worker_init_fn, collate_fn=customBatchBuilder)\n",
    "val_loader = DataLoader(val_folder, batch_size=batch_size, shuffle=True, num_workers=num_workers, worker_init_fn=worker_init_fn, collate_fn=customBatchBuilder)\n",
    "test_loader = DataLoader(test_folder, batch_size=batch_size, shuffle=False, num_workers=num_workers, worker_init_fn=worker_init_fn, collate_fn=customBatchBuilder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cfc7b1a8-7ce8-4c8b-9862-246e150e3272",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_folder.targets\n",
    "class_nums = len(train_folder.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3d5ebdf8-ecca-41c3-a6c1-e3d2f2e5cbe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input_ids shape  torch.Size([32, 6])\n",
      "Sample attention_mask shape  torch.Size([32, 6])\n",
      "Sample pixel_values shape  torch.Size([32, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "inputs = next(iter(train_loader))\n",
    "for key in inputs.keys():\n",
    "  print(\"Sample {} shape \".format(key), inputs[key].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12f416d-5958-4852-b8d9-9ba9bf10bfbc",
   "metadata": {},
   "source": [
    "## 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "99e6fda7-35b0-417e-9124-4b2188dde469",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f2c75337-b3fd-4dba-b97b-8d67c59dd0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_classes_weights(labels, method=\"balanced\"):\n",
    "    classes = np.unique(labels)\n",
    "    nums_list=[len(np.where(labels==cl)[0]) for cl in classes]\n",
    "    print(nums_list)\n",
    "    if method==\"balanced\":\n",
    "        return compute_class_weight(\"balanced\", classes=classes, y=labels)\n",
    "    elif method==\"max\":\n",
    "        # 即用类别中最大样本数量除以当前类别样本的数量，作为权重系数\n",
    "        max_nums = np.max(nums_list)\n",
    "        return [max_nums/nums for nums in nums_list]\n",
    "    elif method==\"reciprocal\":\n",
    "        return [1/nums for nums in nums_list]\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d417699-67a9-49f3-810e-7f128ad9d031",
   "metadata": {},
   "source": [
    "### 02-CLIP-预训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cb04910b-17b4-48b6-ba43-9262c174f5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "model_name = \"xxx\"\n",
    "start_lr = 0.0001\n",
    "\n",
    "loss_img = nn.CrossEntropyLoss()\n",
    "loss_txt = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                            lr=start_lr,\n",
    "                            betas=(0.9,0.999),\n",
    "                            eps=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "80620bf5-9a69-4563-82d9-70e07c555447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # HWC_in 卷积 KKC_inC_out  --> H'W' KKC_in * KKC_in C_out  --> H'W'C_out\n",
    "# # 其中 H'W' 就是 patch 个数，可以看到第二步其实就是在将原始 patch 的得到的特征维度 [KKC_in] --> projection --> C_out\n",
    "# # stride = kernel_size 无重叠 patch\n",
    "# model.visual.conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "772fc016-ec00-421b-ad48-7d558f35113f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model.visual.input_resolution)\n",
    "# # 取 eot_token 作为特征表述\n",
    "# print(\"image feature dim: \", model.visual.proj.shape)\n",
    "# print(\"text feature dim\", model.text_projection.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1203bd-d1c6-4041-8005-a9950daf5220",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "91fef9d5-fe94-44c7-85b6-c5f495783612",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def train(device, model, dataloader, loss_img, loss_txt, train=True, optimizer=None, useproba=True, weights=None, verbose=False):\n",
    "#     correct = 0\n",
    "#     error = 0\n",
    "#     total = 0\n",
    "    \n",
    "#     fin_probas = None\n",
    "#     fin_ls = None\n",
    "    \n",
    "#     for batch, (images, texts) in enumerate(dataloader):\n",
    "#         images, texts = images.to(device), texts.to(device)\n",
    "#         n = images.shape[0]\n",
    "        \n",
    "#         logits_per_image, logits_per_text = model(images, texts)\n",
    "        \n",
    "#         ground_truth = torch.arange(len(images), dtype=torch.long, device=device)\n",
    "#         loss = 0.5 * ( loss_img(logits_per_image, ground_truth) + loss_txt(logits_per_text, ground_truth) )\n",
    "\n",
    "#         if train:\n",
    "#             # 开始优化网络权重\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#         error += loss.item()\n",
    "        \n",
    "#         # 计算准确率\n",
    "#         p = torch.max(logits_per_image,1)[1].to(device)\n",
    "#         correct += (p == ground_truth).sum()\n",
    "#         total += n\n",
    "        \n",
    "#         if verbose:\n",
    "#             pass\n",
    "            \n",
    "#         if useproba:\n",
    "#             probas = logits_per_image.detach().cpu()\n",
    "#             l = ground_truth.detach().cpu()\n",
    "#             fin_probas = probas if fin_probas is None else np.concatenate([fin_probas, probas], axis=0)\n",
    "#             fin_ls = l if fin_ls is None else np.concatenate([fin_ls, l], axis=0)\n",
    "    \n",
    "#     return error / (batch+1), correct / total, fin_probas, fin_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "961247e1-cab0-4a45-b06c-7412cdf3534a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(device, model, dataloader, loss_img, loss_txt, train=True, optimizer=None, useproba=True, weights=None, verbose=False):\n",
    "    correct = 0\n",
    "    error = 0\n",
    "    total = 0\n",
    "    \n",
    "    fin_probas = None\n",
    "    fin_ls = None\n",
    "    \n",
    "    for batch, ipts in enumerate(dataloader):\n",
    "        input_ids, pixel_values, attention_mask = ipts[\"input_ids\"].to(device), ipts[\"pixel_values\"].to(device), ipts[\"attention_mask\"].to(device)\n",
    "        n = input_ids.shape[0]\n",
    "\n",
    "        outputs = model(input_ids, pixel_values, attention_mask)\n",
    "        logits_per_image, logits_per_text = outputs.logits_per_image, outputs.logits_per_text\n",
    "        \n",
    "        ground_truth = torch.arange(n, dtype=torch.long, device=device)\n",
    "        loss = 0.5 * ( loss_img(logits_per_image, ground_truth) + loss_txt(logits_per_text, ground_truth) )\n",
    "\n",
    "        if train:\n",
    "            # 开始优化网络权重\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        error += loss.item()\n",
    "        \n",
    "        # 计算匹配数\n",
    "        p = torch.max(logits_per_image, 1)[1].to(device)\n",
    "        correct += (p == ground_truth).sum()\n",
    "        total += n\n",
    "        \n",
    "        if verbose:\n",
    "            pass\n",
    "            \n",
    "        if useproba:\n",
    "            probas = logits_per_image.detach().cpu()\n",
    "            l = ground_truth.detach().cpu()\n",
    "            fin_probas = probas if fin_probas is None else np.concatenate([fin_probas, probas], axis=0)\n",
    "            fin_ls = l if fin_ls is None else np.concatenate([fin_ls, l], axis=0)\n",
    "    \n",
    "    return error / (batch+1), correct / total, fin_probas, fin_ls\n",
    "\n",
    "def test(device, model, dataloader, loss_img, loss_txt, useproba=True, weights=None, verbose=False):\n",
    "    with torch.no_grad():\n",
    "        return train(device, model, dataloader, loss_img, loss_txt, train=False, optimizer=None, useproba=useproba, weights=weights, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "64ae8b55-4cf0-4a02-9a33-6eea92f317ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_epochs = 300\n",
    "save_epoch_fre = 50\n",
    "save_root = \"./results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "75a53285-452f-4c64-985b-315f4e865187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epoch_lr(cur_epoch):\n",
    "    return cur_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "67431fa7-fc3b-412d-b76a-ba0cb9fe43b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs=[]\n",
    "train_errors=[]\n",
    "train_accs=[]\n",
    "\n",
    "val_errors=[]\n",
    "val_accs=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ce638d-eea5-445a-a9d1-8591eb02998a",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_epoches = range(50, total_epochs, 50)\n",
    "\n",
    "epoch_s = 0\n",
    "epoch_e = total_epochs\n",
    "\n",
    "rows = 1\n",
    "cols = 2\n",
    "\n",
    "for i in range(epoch_s, epoch_e+1):  \n",
    "    if i % save_epoch_fre == 0 and i>0:\n",
    "        state = { 'model': model.state_dict(), 'epoch': i, \"lr\": start_lr}  \n",
    "        path = f\"{save_root}/{model_name}_{i}.pth\"\n",
    "        torch.save(state, path)\n",
    "        \n",
    "    model.train()\n",
    "    train_error,train_acc,train_probas,train_ls = train(device, lora_model, train_loader, criterion, criterion, train=True, optimizer=optimizer, useproba=False)\n",
    "    model.eval()\n",
    "    val_error,val_acc,val_probas,val_ls = test(device, model, val_loader, loss_img, loss_txt, useproba=False)\n",
    "    \n",
    "    idxs.append(i)  \n",
    "    \n",
    "    train_errors.append(train_error) \n",
    "    val_errors.append(val_error)\n",
    "    \n",
    "    train_accs.append(train_acc.cpu().item())  \n",
    "    val_accs.append(val_acc.cpu().item()) \n",
    "    \n",
    "    display.clear_output(wait=True)\n",
    "    \n",
    "    plt.figure(figsize=(cols*5,rows*5))\n",
    "    plt.subplot(rows,cols,1)\n",
    "    plt.plot(idxs,train_errors,c='red',label=\"train_loss\")\n",
    "    plt.plot(idxs,val_errors,c='blue',label=\"val_loss\")\n",
    "    plt.legend(bbox_to_anchor=(1.5, 1), loc=1)\n",
    "\n",
    "    plt.subplot(rows,cols,2)\n",
    "    plt.plot(idxs,train_accs,c='red',label=\"train_acc\")\n",
    "    plt.plot(idxs,val_accs,c='blue',label=\"val_acc\")\n",
    "    plt.legend(bbox_to_anchor=(1.5, 1), loc=1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.pause(0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421ffd62-fcc3-46fa-9e40-4b31509cc80a",
   "metadata": {},
   "source": [
    "## 保存"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcf6176-c57f-4cde-a9de-92cc0e39b85a",
   "metadata": {},
   "source": [
    "## Zero-Shot 效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b4e478-5110-44dd-ba02-1fa0d2ca6c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d699eeef-909b-4104-a99d-50f9651f35d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_top_k_acc(probas, lables, k=1):\n",
    "    max_indics = np.argmax(probas, axis=1)\n",
    "    return len(np.where(max_indics==lables)[0]) / len(lables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c200ebaf-983d-4ca0-8b5a-3837b359ce9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_error,val_acc,val_probas,val_ls = test(device, model, val_loader, loss_img, loss_txt, useproba=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d06170c-b55a-494a-ab01-df14dd59526d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MyDataset(Dataset):\n",
    "#     def __init__(self, dataset_folder, prompts: list[str], context_length: Optional[int]=77):\n",
    "#         print_dataset_folder_info(dataset_folder)\n",
    "        \n",
    "#         self.dataset_folder = dataset_folder\n",
    "#         self.prompts = prompts\n",
    "#         self.context_length = 77 if context_length is None else context_length\n",
    "\n",
    "#         self.targets = dataset_folder.targets\n",
    "#         self.classes = dataset_folder.classes\n",
    "#         self.class_to_idx = dataset_folder.class_to_idx\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.dataset_folder)\n",
    "        \n",
    "#     def __getitem__(self, idx):\n",
    "#         X, y = self.dataset_folder[idx]\n",
    "#         text = clip.tokenize(self.prompts[y], self.context_length).reshape((-1,))\n",
    "#         return X, text, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782a0b2a-f11b-4781-b89e-8a2b49c717a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompts = [\n",
    "#     \"This image suggests a severe anxiety psychological tendency\",  # 重度\n",
    "#     \"This image suggests a mild anxiety psychological tendency\",  # 中度\n",
    "#     \"This image suggests no anxiety psychological tendency\",  # 无\n",
    "# ]\n",
    "\n",
    "prompts = [\n",
    "    \"This is a cat\",  \n",
    "    \"This is a dog\",  \n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
